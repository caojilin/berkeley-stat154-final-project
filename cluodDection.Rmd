---
title: "Proj2_154"
author: "Sizhuo (Cindy) Liu"
date: "4/20/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = "figures/")
# Load Required Packages
library(caret)
library(ggplot2)
library(scales)
library(e1071)
library(class)
library(randomForest)
library(ggfortify)
library(MASS)
library(corrplot)
# decision tree
library(rpart)
library(PRROC)
library(rpart.plot)
```

1. Data Collection and Exploration

```{r}
# Load Image Data & Add Column Names
image1 <- read.table("image_data/image1.txt")
colnames(image1) <- c('y_coordinate', 'x_coordinate', 'expert_label', 'NDAI', 'SD', 'CORR', 'RA_DF', 'RA_CF', 'RA_BF', 'RA_AF', 'RA_AN')
image2 <- read.table("image_data/image2.txt")
colnames(image2) <- c('y_coordinate', 'x_coordinate', 'expert_label', 'NDAI', 'SD', 'CORR', 'RA_DF', 'RA_CF', 'RA_BF', 'RA_AF', 'RA_AN')
image3 <- read.table("image_data/image3.txt")
colnames(image3) <- c('y_coordinate', 'x_coordinate', 'expert_label', 'NDAI', 'SD', 'CORR', 'RA_DF', 'RA_CF', 'RA_BF', 'RA_AF', 'RA_AN')

# Add a column identifying which image they are from (for later convenience when splitting and merging data)
image1$image_origin <- 1

image2$image_origin <- 2
rownames(image2) = 115230:(115229+115110)
image3$image_origin <- 3
rownames(image3) = 230340:(230339+115217)


# Remove zero ('unlabeled') values and convert label from {-1,1} to {0,1}
image1_nozero <- image1[-which(image1$expert_label == 0),]
image1_nozero$expert_label[which(image1_nozero$expert_label == -1)] <- 0
image1_nozero$expert_label <- as.factor(image1_nozero$expert_label)
image1_nozero$expert_label <- droplevels(image1_nozero$expert_label)
# rownames(image1_nozero) <- 1:nrow(image1_nozero)
image2_nozero <- image2[-which(image2$expert_label == 0),]
image2_nozero$expert_label[which(image2_nozero$expert_label == -1)] <- 0
image2_nozero$expert_label <- as.factor(image2_nozero$expert_label)
image2_nozero$expert_label <- droplevels(image2_nozero$expert_label)
#  rownames(image2_nozero) <- 1:nrow(image2_nozero)

image3_nozero <- image3[-which(image3$expert_label == 0),]
image3_nozero$expert_label[which(image3_nozero$expert_label == -1)] <- 0
image3_nozero$expert_label <- as.factor(image3_nozero$expert_label)
image3_nozero$expert_label <- droplevels(image3_nozero$expert_label)
# rownames(image3_nozero) <- 1:nrow(image3_nozero)

images_all <- rbind(image1_nozero, image2_nozero, image3_nozero)

```

```{r, eval=F}
# 1 b) Plot Well-Labeled Map
ggplot(data = image1) + geom_point(aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label))) + scale_y_reverse() + scale_color_manual("Expert Labels", labels = c("Not Cloud", "No Label", "Cloud"), values = c("grey", "black", "white")) + ggtitle('Image 1: MISR Orbit 13257')
ggplot(data = image2) + geom_point(aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label))) + scale_y_reverse() + scale_color_manual("Expert Labels", labels = c("Not Cloud", "No Label", "Cloud"), values = c("grey", "black", "white")) + ggtitle('Image 2: MISR Orbit 13490')
ggplot(data = image3) + geom_point(aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label))) + scale_y_reverse() + scale_color_manual("Expert Labels", labels = c("Not Cloud", "No Label", "Cloud"), values = c("grey", "black", "white")) + ggtitle('Image 3: MISR Orbit 13723')
```

```{r, eval=F}
# 1 c)
# Observe pair-wise relationships between variables 
pairs(images_all[,c(7, 8, 9, 10, 11)])

corrplot(cor(images_all[,c(7, 8, 9, 10, 11)]), 
         method = 'number', 
         type = 'lower',
         tl.srt = 45,
         title = "Correlation For Radiance Angles")
pairs(image1_nozero[,c(4, 5, 6, 7)])
pairs(image1_nozero[,c(4, 5, 6)], main = "Pairwise Plot for NDAI, SD, CORR in Image 1")
pairs(image2_nozero[,c(4, 5, 6)])
pairs(image3_nozero[,c(4, 5, 6)])
pairs(images_all[,c(4,5,6)], main = "Pairwise Plot for NDAI, SD, CORR")


# Relationship between CORR and NDAI color-coded with expert labels
ggplot(data = image1_nozero) + geom_point(aes(x = CORR, y = NDAI, color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red")) + coord_flip()
ggplot(data = image2_nozero) + geom_point(aes(x = CORR, y = NDAI, color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red")) + coord_flip()
ggplot(data = image3_nozero) + geom_point(aes(x = CORR, y = NDAI, color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red")) + coord_flip()

# Relationship between SD and NDAI color-coded with expert labels
ggplot(data = image1_nozero) + geom_point(aes(x = SD, y = NDAI, color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red")) + coord_flip()
ggplot(data = image2_nozero) + geom_point(aes(x = SD, y = NDAI, color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red")) + coord_flip()
ggplot(data = image3_nozero) + geom_point(aes(x = SD, y = NDAI, color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red")) + coord_flip()

# Relationship between CORR and SD color-coded with expert labels
ggplot(data = image1_nozero) + geom_point(aes(x = SD, y = CORR, color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red")) + coord_flip()
ggplot(data = image2_nozero) + geom_point(aes(x = SD, y = CORR, color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red")) + coord_flip()
ggplot(data = image3_nozero) + geom_point(aes(x = SD, y = CORR, color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red")) + coord_flip()

# Relationship between CORR and AF, AN, BF Angles
ggplot(data = images_all) + geom_point(aes(x = RA_AF, y = CORR)) + ggtitle ('CORR Plotted Against Radiance Angle AF')
ggplot(data = images_all) + geom_point(aes(x = RA_AN, y = CORR)) + ggtitle ('CORR Plotted Against Radiance Angle AN')
ggplot(data = images_all) + geom_point(aes(x = RA_BF, y = CORR)) + ggtitle ('CORR Plotted Against Radiance Angle BF')

# Distribution of NDAI In Relation to Expert Label
ggplot(data = image1_nozero) + geom_histogram(aes(x = image1_nozero$NDAI, fill = expert_label))
ggplot(data = image2_nozero) + geom_histogram(aes(x = image2_nozero$NDAI, fill = expert_label))
ggplot(data = image3_nozero) + geom_histogram(aes(x = image3_nozero$NDAI, fill = expert_label))
ggplot(data = images_all) + geom_histogram(aes(x = images_all$NDAI, fill = expert_label)) + ggtitle('Distribution of NDAI')

# Distribution of CORR In Relation to Expert Label
ggplot(data = image1_nozero) + geom_histogram(aes(x = image1_nozero$CORR, fill = expert_label))
ggplot(data = image2_nozero) + geom_histogram(aes(x = image2_nozero$CORR, fill = expert_label))
ggplot(data = image3_nozero) + geom_histogram(aes(x = image3_nozero$CORR, fill = expert_label))
ggplot(data = images_all) + geom_histogram(aes(x = images_all$CORR, fill = expert_label)) + ggtitle('Distribution of CORR')
# Distribution of SD In Relation to Expert Label
ggplot(data = image1_nozero) + geom_histogram(aes(x = image1_nozero$SD, fill = expert_label))
ggplot(data = image2_nozero) + geom_histogram(aes(x = image2_nozero$SD, fill = expert_label))
ggplot(data = image3_nozero) + geom_histogram(aes(x = image3_nozero$SD, fill = expert_label))
ggplot(data = images_all) + geom_histogram(aes(x = images_all$SD, fill = expert_label)) + ggtitle('Distribution of SD')
# Distribution of RA_DF In Relation to Expert Label
ggplot(data = image1_nozero) + geom_histogram(aes(x = image1_nozero$RA_DF, fill = expert_label))
ggplot(data = image2_nozero) + geom_histogram(aes(x = image2_nozero$RA_DF, fill = expert_label))
ggplot(data = image3_nozero) + geom_histogram(aes(x = image3_nozero$RA_DF, fill = expert_label))
ggplot(data = images_all) + geom_histogram(aes(x = images_all$RA_DF, fill = expert_label)) + ggtitle('Distribution of Radiance Angle DF')

```

#2. Preparation

#2 a)

```{r}
# Non-trivial Method 1: split each image into many smaller squares and randomly select some squares to be testing and others to be training 

split_image1_64 <- split_squares(image1_nozero, 64)
split_image2_64 <- split_squares(image2_nozero, 64)
split_image3_64 <- split_squares(image3_nozero, 64)

split_three <- function(df, n) {
  training <- data.frame()
  testing <- data.frame()
  validation <- data.frame()
  test_indices <- sample(n * n, floor(n * n * 0.2))
  train_validation_indices <- c(1: (n*n))[-test_indices]
  validation_indices <- sample(train_validation_indices, floor(length(train_validation_indices) * 0.2))
  train_indices <- train_validation_indices[-validation_indices]
  for (i in 1:length(test_indices)) {
    testing<- rbind(testing, df[[test_indices[i]]])
    i <- i + 1
  }
  for (j in 1:length(train_indices)) {
    training <- rbind(training, df[[train_indices[j]]])
    j <- j + 1
  }
  for (h in 1:length(validation_indices)) {
    validation <- rbind(validation, df[[validation_indices[h]]])
  }
  list <- list(test = testing, train = training, validation = validation)
  return (list)
} 


image1_all <- split_three(split_image1_64, 64)
image2_all <- split_three(split_image2_64, 64)
image3_all <- split_three(split_image3_64, 64)

trainM2 <- rbind(image1_all$train, 
               image2_all$train, image3_all$train)

validationM2 <- rbind(image1_all$validation,
                    image2_all$validation,
                    image3_all$validation)

testM2 <- rbind(image1_all$test,
                    image2_all$test,
                    image3_all$test)

### finalized
testing <- unique(testM2)

train_validation <- unique(rbind(trainM2,validationM2))

```

```{r}
# Non-Trivial Split Method 2: Split each image into 16 squares, extract 80% training, and 20% testing from each square
split_squares <- function(df, n) {
  list <- c()
  x_square_coordinates <- seq(range(df$x_coordinate)[1], range(df$x_coordinate)[2], length.out = n + 1)
  y_square_coordinates <- seq(range(df$y_coordinate)[1], range(df$y_coordinate)[2], length.out = n + 1)
  index = 1
  for (i in 1:n) {
    for (j in 1:n) {
      list[[index]] = df[df$y_coordinate <= y_square_coordinates[length(y_square_coordinates) - j + 1] 
                     & df$y_coordinate >= y_square_coordinates[length(y_square_coordinates) - j] 
                     & df$x_coordinate <= x_square_coordinates[i + 1] 
                     & df$x_coordinate >= x_square_coordinates[i] , ]
      index = index + 1
    }
  }
  return (list)
}

split_image1_4 <- split_squares(image1_nozero, 4)
split_image2_4 <- split_squares(image2_nozero, 4)
split_image3_4 <- split_squares(image3_nozero, 4)

split <- function(df) {
  Train = createDataPartition(1:nrow(df), p=0.8, list = FALSE)
  training <-df[ Train, ]
  testing <- df[ -Train, ]
  val = createDataPartition(1:nrow(training), p=0.8, list = FALSE)
  training = training[val,]
  validation = training[-val,]
  return(list(train = training, validation = validation, test = testing))
}

split_one <- function (image, n) {
  train1 <- data.frame()
  validation1 <- data.frame()
  test1 <- data.frame()
  for (i in 1:n) {
    split_three <- split(split_squares(image, sqrt(n))[[i]])
    train1 <- rbind(train1, split_three[[1]])
    validation1 <- rbind(validation1, split_three[[2]])
    test1 <- rbind(test1, split_three[[3]])
  }
  return (list(train = train1, validation = validation1, test = test1))
}

training <- data.frame()
validation <- data.frame()
testing <- data.frame()
training <- rbind(split_one(image1_nozero, 16)$train, split_one(image2_nozero, 16)$train,
                  split_one(image3_nozero, 16)$train)
validation <- rbind(split_one(image1_nozero, 16)$validation, split_one(image2_nozero, 16)$validation,
                  split_one(image3_nozero, 16)$validation)
testing <- rbind(split_one(image1_nozero, 16)$test, split_one(image2_nozero, 16)$test,
                  split_one(image3_nozero, 16)$test)


### finalized
train_validationM2 <- unique(rbind(training, validation))
testM2 <- unique(testing)
```





```{r, eval = FALSE}
# Method 3: Even more naive split
test_image1 <- image1_nozero[image1_nozero$y_coordinate <= 100,]
test_image2 <- image2_nozero[image2_nozero$y_coordinate <= 100,]
test_image3 <- image3_nozero[image3_nozero$y_coordinate <= 100,]
val_train_image1 <- image1_nozero[image1_nozero$y_coordinate > 100,]
val_train_image2 <- image2_nozero[image2_nozero$y_coordinate > 100,]
val_train_image3 <- image3_nozero[image3_nozero$y_coordinate > 100,]
train_validationM3 <- rbind(val_train_image1, val_train_image2, val_train_image3)
testM3 <- rbind(test_image1, test_image2, test_image3)
```


# 2 (b) baseline accuracy

The trivial classifier will have a high accuracy when the original image has mostly clear areas (without clouds).
- Method 1 is preferred over Method 2.
- The way method 1 is constructed ensures that in training, validation and test sets, there are equal amounts of data from each image. 
- The table at the end of Method 2, however, suggests that the random selection tends to contain more data from one image than another.

```{r}
baseline_preds <- 0
# validation accuracy
val_accuracy <- mean(baseline_preds == validation$expert_label)
# test accuracy
test_accuracy <-  mean(baseline_preds == testing$expert_label)

data.frame('Validation_Accuracy' = val_accuracy, 'Test_Accuracy' = test_accuracy)
```

```{r, eval = False}
# c)
# Logistic 

# PCA
image1_pca <- prcomp(image1_nozero[,c(7:11)], center = TRUE, scale. = TRUE)
autoplot(image1_pca, data = image1_nozero, loadings = TRUE, loadings.label = TRUE)

train_pca <- prcomp(training[,c(7:11)], center = TRUE, scale. = TRUE)
PC1 <- train_pca$x[,1]
PC2 <- train_pca$x[,2]
PC3 <- train_pca$x[,3]
pca_train <- data.frame(expert_label = training$expert_label, PC1, PC2, PC3)
pca_logistic <- glm(expert_label ~ PC1 + PC2 + PC3,
                    data = pca_train,
                    family = "binomial")
validation_pca <- prcomp(validation[,c(7:11)], center = TRUE, scale. = TRUE)
PC1 <- validation_pca$x[,1]
PC2 <- validation_pca$x[,2]
PC3 <- validation_pca$x[,3]
validation.pca <- data.frame(expert_label = validation$expert_label, PC1, PC2, PC3)
y_hat_pca <- predict(pca_logistic, newdata = validation.pca, type = 'response')
y_hat_pca <- y_hat_pca > 0.5
y_hat_pca[which(y_hat_pca == FALSE)] <- 0
y_hat_pca[which(y_hat_pca == TRUE)] <- 1
mean(y_hat_pca  == validation$expert_label)
var(y_hat_pca  == validation$expert_label)

# Scree Plot
eigenvalues <- summary(image1_pca)$importance[3,]
ggplot() + geom_point(aes(x = 1:length(eigenvalues), y=eigenvalues)) + geom_path(aes(x = 1:length(eigenvalues), y=eigenvalues)) + labs(x = "PCs", y = "Fraction of Total Variance Explained") + ggtitle('Scree Plot of the Eigenvalues')

# RA Angles 
RA_mod <- glm(expert_label ~ RA_AN + RA_AF + RA_BF,
              data = train_validation,
              family = "binomial")
y_hat_RA <- predict(RA_mod, newdata = testing, type = 'response')
y_hat_RA <- y_hat_RA > 0.5
y_hat_RA[which(y_hat_RA == FALSE)] <- 0
y_hat_RA[which(y_hat_RA == TRUE)] <- 1
mean(y_hat_RA == testing$expert_label)
var(y_hat_RA == testing$expert_label)


# SD, CORR, NDAI 
paper_mod <- glm(expert_label ~ SD + CORR + NDAI,
              data = train_validation,
              family = "binomial")
y_hat_paper <- predict(paper_mod, newdata = testing, type = 'response')
y_hat_paper <- y_hat_paper > 0.5
y_hat_paper[which(y_hat_paper == FALSE)] <- 0
y_hat_paper[which(y_hat_paper == TRUE)] <- 1
mean(y_hat_paper == testing$expert_label)
var(y_hat_paper == testing$expert_label)
image1_nozero

# xy coordinates
location_mod <- glm(expert_label ~ x_coordinate + y_coordinate,
              data = train_validation,
              family = "binomial")
y_hat_location <- predict(location_mod, newdata = testing, type = 'response')
y_hat_location <- y_hat_location > 0.5
y_hat_location[which(y_hat_location == FALSE)] <- 0
y_hat_location[which(y_hat_location == TRUE)] <- 1
mean(y_hat_location == testing$expert_label)
var(y_hat_location == testing$expert_label)

# randomly choose three features 
highest_accuracy <- mean(y_hat_paper == testing$expert_label)
lowest_variance <- var(y_hat_paper == testing$expert_label)
i <- 0
for (i in 1: 100) {
  explanatory <- colnames(image1_nozero)[-c(1:3)]
var_indices <- sample(1:length(explanatory), 3)
formula <- paste('expert_label', ' ~ ', paste(explanatory[var_indices], collapse = ' + '))
mod <- glm(formula, data = train_validation, family = "binomial")
hat <- predict(mod, newdata = testing, type = 'response')
hat <- hat > 0.5
hat[which(hat == FALSE)] <- 0
hat[which(hat == TRUE)] <- 1
accuracy <- mean(hat == testing$expert_label)
variance <- var(hat == testing$expert_label)
i <- i + 1
if (accuracy >= highest_accuracy & variance <= lowest_variance) {
  break
}
}
highest_accuracy
accuracy
lowest_variance
variance
mod
```

# 2 d) generic CV function
fix --> four folds, four numbers
```{r}
# d)
CVgeneric <- function(classifier, features, labels, K=5, loss_fn=NULL, 
                      model_input) {
  folds <- createFolds(1:nrow(features), k = K)
  output <- data.frame(Folds = c(1:K), accuracy = rep(0, K), loss= rep(0, K))
  loss <- rep(0, K)
  accuracy = rep(0, K)
  dat = cbind(features, labels)
  for (i in 1: K) {
    train.cv = dat[-folds[[i]],]
    test.cv = dat[folds[[i]],]
    train.label = labels[-folds[[i]],]
    test.label = labels[folds[[i]],]
    if (classifier == "logistic") {
       mod_fit = glm(model_input, data = train.cv, family = "binomial")
       y_hat = predict(mod_fit, test.cv, type="response")
       y_hat = y_hat > 0.5
       y_hat[which(y_hat == FALSE)] <- 0
       y_hat[which(y_hat == TRUE)] <- 1
       
    }else if (classifier == "qda") {
        mod_fit <- qda(model_input, data = train.cv)
        y_hat = predict(mod_fit, test.cv)$class
    }else if (classifier == "lda") {
        mod_fit <- lda(model_input, data = train.cv)
        y_hat = predict(mod_fit, test.cv)$class
    }else{
        mod_fit <- train(model_input, data = train.cv, method=classifier)
        y_hat = predict(mod_fit, test.cv)
    } 
    # output[i,3] = loss_fn(y_hat, train.label)
    output[i,2] = mean(y_hat == test.label)
    }
  return(output)
}
```

# 3. Modeling


```{r}
# Logistic Regression Model 
model_input = formula("expert_label ~ SD + CORR + NDAI")
train.feature.m1 = train_validation[,c(4,5,6)]
train.label.m1 = data.frame(expert_label = train_validation[,3])

train.feature.m2 = train_validationM2[,c(4,5,6)]
train.label.m2 = data.frame(expert_label = train_validationM2[,3])
# Split 1
output1_logistic = CVgeneric("logistic", train.feature.m1, train.label.m1, K=5, model_input=model_input)

# Split 2
output2_logistic = CVgeneric("logistic", train.feature.m2, train.label.m2, K=5, model_input=model_input)

write.csv(output1_logistic, 'output1_logistic.csv')
```

```{r}
# QDA

# Split 1
output1_qda = CVgeneric("qda", train.feature.m1, train.label.m1, K=5, model_input=model_input)

# Split 2
output2_qda = CVgeneric("qda", train.feature.m2, train.label.m2, K=5, model_input=model_input)

output1_qda
output2_qda
```

```{r}
# LDA
# Split 1
output1_lda = CVgeneric("lda", train.feature.m1, train.label.m1, K=5, model_input=model_input)

# Split 2
output2_lda = CVgeneric("lda", train.feature.m2, train.label.m2, K=5, model_input=model_input)

output1_lda
output2_lda
```

```{r}
# KNN

CV4 <- function(data, K, k_num) {
    folds <- createFolds(1:nrow(data), k = K)
    output <- data.frame(Folds = c(1:K), accuracy = rep(0, K), loss= rep(0, K))
    for (i in 1: K) {
      test <- data[folds[[i]],]
      train.cv <- data[-folds[[i]],]
      
      train_filtered <- train.cv[,c(4,5,6)]
      test_filtered <- test[,c(4,5,6)]
      
      y_hat <- knn(train_filtered, test_filtered, train.cv$expert_label, k = k_num)
      output[i,2] = mean(y_hat == test$expert_label)
    }
    return (output)
}

# Split 1
CV4(train_validation, K = 5, k_num = 10)

# Split 2
CV4(train_validationM2, K = 5, k_num = 10)
```

```{r eval = FALSE}
# SVM
# Split 1
output1 = CVgeneric("svm", train.feature.m1, train.label.m1, K=5, model_input=model_input)

# Split 2
output2 = CVgeneric("svm", train.feature.m2, train.label.m2, K=5, model_input=model_input)

```


```{r}
CV6 <- function(data, K, ntree) {
    folds <- createFolds(1:nrow(data), k = K)
    output <- data.frame(Folds = c(1:K), accuracy = rep(0, K), loss= rep(0, K))
    for (i in 1: K) {
      test <- data[folds[[i]],]
      train.cv <- data[-folds[[i]],]
      
      fit_randomForest <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = train.cv,
                   ntree = ntree)
      y_hat = predict(fit_randomForest, test)
      
      output[i,2] = mean(y_hat == test$expert_label)
    }
    return (output)
}

# Split 1
CV6(train_validation, K = 5, ntree = 32)

# Split 2
CV6(train_validationM2, K = 5, ntree=3)
```


#3 b)
```{r, eval = FALSE}
# Compute AUC for predicting Class with the model

# logistic regression
system.time(logistic_mod <- glm(expert_label ~ SD + CORR + NDAI,
                       data = train_validation,
                       family = "binomial"))
y_hat_logistic <- predict(logistic_mod, newdata = testing, type="response")

# https://stats.stackexchange.com/questions/10501/calculating-aupr-in-r
fg1 <- y_hat_logistic[testing$expert_label == 1]
bg1 <- y_hat_logistic[testing$expert_label == 0]

roc1 = roc.curve(fg1, bg1, curve = T)
pr1 = pr.curve(fg1, bg1, curve = T)
plot(roc1)
plot(pr1)
# y_hat_logistic <- y_hat_logistic > 0.5
# y_hat_logistic[which(y_hat_logistic == FALSE)] <- 0
# y_hat_logistic[which(y_hat_logistic == TRUE)] <- 1
# confusionMatrix(data = as.factor(y_hat_logistic), testing$expert_label)

# qda
qda_mod <- qda(expert_label ~ SD + CORR + NDAI, 
                     data = train_validation)
y_hat_qda = predict(qda_mod, testing)$posterior[,1]

fg2 <- y_hat_qda[testing$expert_label == 0]
bg2 <- y_hat_qda[testing$expert_label == 1]

roc2 = roc.curve(fg2, bg2, curve = T)
pr2 = pr.curve(fg2, bg2, curve = T)

# lda
lda_mod <- lda(expert_label ~ SD + CORR + NDAI, 
                     data = train_validation)
y_hat_lda = predict(lda_mod, testing)$posterior[,1]

fg3 <- y_hat_lda[testing$expert_label == 0]
bg3 <- y_hat_lda[testing$expert_label == 1]

roc3 = roc.curve(fg3, bg3, curve = T)
pr3 = pr.curve(fg3, bg3, curve = T)

# knn
train_filtered <- train_validation[,c(4,5,6)]
test_filtered <- testing[,c(4,5,6)]

y_hat_knn = knn(train_filtered, test_filtered, train_validation$expert_label, k = 10,
                     prob = TRUE)

y_hat_knn <- attr(y_hat_knn, "prob")

fg4 <- y_hat_knn[testing$expert_label == 0]
bg4 <- y_hat_knn[testing$expert_label == 1]

roc4 = roc.curve(fg4, bg4, curve = T)
pr4 = pr.curve(fg4, bg4, curve = T)



# random forests
system.time(fit_randomForest <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = train_validation, importance= T,
                   ntree = 3))

y_hat_rf <- predict(fit_randomForest, testing, type = "prob")
y_hat_rf = y_hat_rf[,1]

fg5 <- y_hat_rf[testing$expert_label == 0]
bg5 <- y_hat_rf[testing$expert_label == 1]

roc5 = roc.curve(fg5, bg5, curve = T)
pr5 = pr.curve(fg5, bg5, curve = T)

```

```{r, eval = False}
par(mfrow=c(1,3))
plot(women)
plot(mtcars$mpg, mtcars$cyl)
plot(roc1, main = "Logistic model Roc curve")
plot(pr1, main = "Logistic model PR curve")
plot(roc2, main = "LDA model Roc curve")
plot(pr2, main = "LDA model PR curve")
plot(roc3, main = "QDA model Roc curve")
plot(pr3, main = "QDA model PR curve")
plot(roc4, main = "knn model Roc curve")
plot(pr4, main = "knn model PR curve")
plot(roc5, main = "Decision Tree model Roc curve")
plot(pr5, main = "Decision Tree PR curve")
```


#3c
```{r, eval = False}
# use three decision trees to examine three images
rpart.plot(rpart(expert_label ~ SD + CORR + NDAI, 
                   data = image1_nozero))

rpart.plot(rpart(expert_label ~ SD + CORR + NDAI, 
                   data = image2_nozero))

rpart.plot(rpart(expert_label ~ SD + CORR + NDAI, 
                   data = image3_nozero))
```

4. Diagnostics

a) 
- Random forest: accuracy converge to a certain value as ntree goes infinitely high
- Knn: same as above for k value
- Logistic Regression: Beta

```{r eval = FALSE}
#don't run on labtop, too slow
#I run it on a cloud computer
# Find optimal parameter for random forest 
mean_accuracy = rep(0, 30)
times = rep(0, 30)
n_tree = c(seq(1,40,2),seq(40,200,20),250)
for(i in 1:30){
  t = system.time(fit_randomForest <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = training,
                   ntree = n_tree[i]))
  times[i] = round(t[1], 5)
  y_hat <- predict(fit_randomForest, validation)
  mean_accuracy[i] = mean(y_hat == validation$expert_label)
}


# find optimal k for knn
K = 30
mean_accuracy = rep(0, K)
for(i in 1:K){
  train_filtered <- training[,c(4,5,6)]
  train.label = training[,3]
  test_filtered <- validation[,c(4,5,6)]
  test.label = validation[,3]
  y_hat = knn(train_filtered, test_filtered, train.label, k = i,
                       prob = TRUE)
  mean_accuracy[i] = mean(y_hat == test.label)
}
```


b) extract misclassification data
```{r, eval = False}
# Chosen classifier: random forest: use ntree = 32 for now NEED TO CHANGE
# split 1
randomforest_mod <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = train_validation,
                   ntree = 1)
randomforest_y_hat <- predict(randomforest_mod, testing)
misclassified1 <- testing[randomforest_y_hat != testing$expert_label,]
plot(misclassified1$x_coordinate, misclassified1$y_coordinate)

ggplot(data = misclassified1) + geom_point(aes(x = x_coordinate, y = y_coordinate, color = image_origin)) + scale_y_reverse()
# more errors were made on the edges (when y coordinate value was low
boxplot(misclassified1$NDAI, train_validation$NDAI, names = c("Misclassified", "Train_Validation"), 
        horizontal = TRUE)
boxplot(misclassified1$CORR, train_validation$CORR, names = c("Misclassified", "Train_Validation"),
        horizontal = TRUE)
boxplot(misclassified1$SD, train_validation$SD, names = c("Misclassified", "Train_Validation"),
        horizontal = TRUE)
plot(misclassified1$expert_label)
# more errors were made by identifying an area to be clear when there is in fact cloud 

# split 2
randomforest_mod2 <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = train_validationM2,
                   ntree = 32)
randomforest_y_hat2 <- predict(randomforest_mod2, testing)
misclassified2 <- testing[randomforest_y_hat2 != testing$expert_label,]
plot(misclassified2$x_coordinate, misclassified2$y_coordinate)
plot(misclassified2$expert_label)
# more errors were made by identifying an area to be clear when there is in fact cloud 





```


```{r, eval = False}
mod <- lda(expert_label ~ SD + CORR + NDAI, 
                   data = train_validation)
y_hat <- predict(mod, testing)
misclassified1 <- testing[y_hat$class != testing$expert_label,]

mod <- lda(expert_label ~ SD + CORR + NDAI, 
                   data = train_validationM2)
y_hat <- predict(mod, testM2)
misclassified1 <- testing[y_hat$class != testM2$expert_label,]
```



```{r, eval = False}
# Image2 Misclassified Distribution

image1_misclassified1 <- misclassified1[misclassified1$image_origin == 1,]
df2 <- image1
predicted_to_0_indices_2 <- as.integer(rownames(image1_misclassified1[image1_misclassified1$expert_label == 1, ]))
predicted_to_1_indices_2 <- as.integer(rownames(image1_misclassified1[image1_misclassified1$expert_label == 0, ]))
# 1 cloud , 0 no label, -1 clear, 2 predicted to -1, -2 predicted to 1,
df2$expert_label[which(as.integer(rownames(df2)) %in% predicted_to_0_indices_2)] <- 2
df2$expert_label[which(as.integer(rownames(df2)) %in% predicted_to_1_indices_2)] <- -2

# Graph
ggplot() + geom_point(data = df2, aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label))) + scale_y_reverse() + scale_color_manual("Expert Labels", labels = c("Misclassified to Cloudy", "Clear", "No Label", "Cloudy", "Misclassified to Clear"), values = c("red", "grey", "black", "white", "blue"))  + geom_point(data = df2[df2$expert_label == -2 | df2$expert_label == 2,], aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label)), size = 1) + ggtitle('Distribution of Misclassification Errors (Image 1, LDA, Split Method 2)')
```


# 4d)
```{r}
# knn splitting method 1 on testing 
train_filtered <- train_validation[,c(4,5,6)]
test_filtered <- testing[,c(4,5,6)]

y_hat_knn = knn(train_filtered, test_filtered, train_validation$expert_label, k = 1,
                     prob = TRUE)

misclassified1 <- testing[y_hat_knn != testing$expert_label,]

# knn splitting method 2 on testing 
train_filtered <- train_validationM2[,c(4,5,6)]
test_filtered <- testM2[,c(4,5,6)]

y_hat_knn <- knn(train_filtered, test_filtered, train_validationM2$expert_label, k = 1,
                     prob = TRUE)

misclassified1 <- testM2[y_hat_knn != testM2$expert_label,]
```









