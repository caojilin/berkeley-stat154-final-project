---
title: "Proj2_154"
author: "Sizhuo (Cindy) Liu"
date: "4/20/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load Required Packages
library(caret)
library(ggplot2)
library(scales)
library(e1071)
library(class)
library(randomForest)
library(ggfortify)
library(MASS)
# decision tree
library(rpart)
library(PRROC)
library(rpart.plot)
```

1. Data Collection and Exploration

The purpose of the study is to to propose 

```{r}
# Load Image Data & Add Column Names
image1 <- read.table("image_data/image1.txt")
colnames(image1) <- c('y_coordinate', 'x_coordinate', 'expert_label', 'NDAI', 'SD', 'CORR', 'RA_DF', 'RA_CF', 'RA_BF', 'RA_AF', 'RA_AN')
image2 <- read.table("image_data/image2.txt")
colnames(image2) <- c('y_coordinate', 'x_coordinate', 'expert_label', 'NDAI', 'SD', 'CORR', 'RA_DF', 'RA_CF', 'RA_BF', 'RA_AF', 'RA_AN')
image3 <- read.table("image_data/image3.txt")
colnames(image3) <- c('y_coordinate', 'x_coordinate', 'expert_label', 'NDAI', 'SD', 'CORR', 'RA_DF', 'RA_CF', 'RA_BF', 'RA_AF', 'RA_AN')

# Remove zero ('unlabeled') values and convert label from {-1,1} to {0,1}

image1_nozero <- image1[-which(image1$expert_label == 0),]
image1_nozero$expert_label[which(image1_nozero$expert_label == -1)] <- 0
image1_nozero$expert_label <- as.factor(image1_nozero$expert_label)
image1_nozero$expert_label <- droplevels(image1_nozero$expert_label)
rownames(image1_nozero) <- 1:nrow(image1_nozero)
image2_nozero <- image2[-which(image2$expert_label == 0),]
image2_nozero$expert_label[which(image2_nozero$expert_label == -1)] <- 0
image2_nozero$expert_label <- as.factor(image2_nozero$expert_label)
image2_nozero$expert_label <- droplevels(image2_nozero$expert_label)
rownames(image2_nozero) <- 1:nrow(image2_nozero)

image3_nozero <- image3[-which(image3$expert_label == 0),]
image3_nozero$expert_label[which(image3_nozero$expert_label == -1)] <- 0
image3_nozero$expert_label <- as.factor(image3_nozero$expert_label)
image3_nozero$expert_label <- droplevels(image3_nozero$expert_label)
rownames(image3_nozero) <- 1:nrow(image3_nozero)
```

```{r, eval=F}
# 1 a) lot Well-Labeled Map
ggplot(data = image1) + geom_point(aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label))) + scale_y_reverse() + scale_color_manual("Expert Labels", labels = c("Not Cloud", "No Label", "Cloud"), values = c("grey", "black", "white"))
ggplot(data = image2) + geom_point(aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label))) + scale_y_reverse() + scale_color_manual("Expert Labels", labels = c("Not Cloud", "No Label", "Cloud"), values = c("grey", "black", "white"))
ggplot(data = image3) + geom_point(aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label))) + scale_y_reverse() + scale_color_manual("Expert Labels", labels = c("Not Cloud", "No Label", "Cloud"), values = c("grey", "black", "white")) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "No Label", "Cloud"), values = c("grey", "black", "white"))
```

```{r, eval=F}
# Observe pair-wise relationships between variables ??? Plots different from in the paper
# pairs(image1_nozero[,c(4, 5, 6)])
# pairs(image2_nozero[,c(4, 5, 6)])

ggplot(data = image1_nozero) + geom_point(aes(x = CORR, y = rescale(image1_nozero$NDAI, to = c(-0.5, 1)), color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red")) + coord_flip()

ggplot(data = image2_nozero) + geom_point(aes(x = CORR, y = NDAI, color = as.factor(expert_label))) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red"))

ggplot(data = image2_nozero) + geom_histogram(aes(x = rescale(image2_nozero$NDAI, to = c(-0.5, 1))))

```

```{r, eval=F}
# Pairwise Relationships with Expert Label 
ggplot(data = image1_nozero) + geom_boxplot(aes(y = CORR, color = expert_label))
boxplot(image1_nozero$CORR)

ggplot(data = image1_nozero) + geom_histogram(aes(x = CORR, color = expert_label, fill = expert_label, alpha = 0.5))

ggplot(data = image1_nozero) + geom_histogram(aes(x = SD, color = expert_label, fill = expert_label, alpha = 0.5)) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red"))

ggplot(data = image1_nozero) + geom_histogram(aes(x = NDAI, color = expert_label, fill = expert_label, alpha = 0.5)) + scale_color_manual("Expert Labels", labels = c("Not Cloud", "Cloud"), values = c("blue", "red"))
```

#2. Preparation

#2 a)
```{r}
# Method 1
split_squares <- function(df) {
  y_steps <- seq(range(df$y_coordinate)[1], 
                   range(df$y_coordinate)[2], length.out = 3)
  x_steps <- seq(range(df$x_coordinate)[1], 
                   range(df$x_coordinate)[2], length.out = 3)
  
  df1 <- df[df$y_coordinate >= y_steps[1] 
                     & df$y_coordinate < y_steps[2]
                     & df$x_coordinate >= x_steps[1]
                     & df$x_coordinate < x_steps[2], ]

  df2 <- df[df$y_coordinate >= y_steps[2] 
                       & df$y_coordinate < y_steps[3]
                       & df$x_coordinate >= x_steps[1]
                       & df$x_coordinate < x_steps[2], ]
  
  df3 <- df[df$y_coordinate >= y_steps[1] 
                       & df$y_coordinate < y_steps[2]
                       & df$x_coordinate >= x_steps[2]
                       & df$x_coordinate < x_steps[3], ]
  
  df4 <- df[df$y_coordinate >= y_steps[2] 
                       & df$y_coordinate < y_steps[3]
                       & df$x_coordinate >= x_steps[2]
                       & df$x_coordinate < x_steps[3], ]
return (list(lower_left = df1, upper_left = df2, lower_right = df3, upper_right = df4))
}

split <- function(df) {
  Train = createDataPartition(1:nrow(df), p=0.8, list = FALSE)
  training <-df[ Train, ]
  testing <- df[ -Train, ]
  val = createDataPartition(1:nrow(training), p=0.8, list = FALSE)
  training = training[val,]
  validation = training[-val,]
  return(list(train = training, validation = validation, test = testing))
}

split_one <- function (image) {
  train1 <- data.frame()
  validation1 <- data.frame()
  test1 <- data.frame()
  for (i in 1:4) {
    split_three <- split(split_squares(image)[[i]])
    train1 <- rbind(train1, split_three[[1]])
    validation1 <- rbind(validation1, split_three[[2]])
    test1 <- rbind(test1, split_three[[3]])
  }
  return (list(train = train1, validation = validation1, test = test1))
}

training <- data.frame()
validation <- data.frame()
testing <- data.frame()
training <- rbind(split_one(image1_nozero)$train, split_one(image2_nozero)$train,
                  split_one(image3_nozero)$train)
validation <- rbind(split_one(image1_nozero)$validation, split_one(image2_nozero)$validation,
                  split_one(image3_nozero)$validation)
testing <- rbind(split_one(image1_nozero)$test, split_one(image2_nozero)$test,
                  split_one(image3_nozero)$test)
```


```{r}
# a) Data Split

# Method 2: Split the data from each image individually and then combine 
# Andrew Ng's Recommendation: Training: 60%, Validation: 20%, Testing: 20%

split_set <- function(data, num) {
    data$image <- rep(num, nrow(data))
    all_indices <- 1:nrow(data)
    test_index <- sample(all_indices, nrow(data) * 0.2)
    remaining_index <- all_indices[-test_index]
    training_index <- sample(remaining_index, length(remaining_index) * 0.8)
    validation_index <- remaining_index[-training_index]
    train <- data[training_index,]
    validation <- data[validation_index,]
    test <- data[test_index,]
    return(list(train = train,validation = validation,test = test))
}

data1 <- split_set(image1_nozero, 1)
data2 <- split_set(image2_nozero, 2)
data3 <- split_set(image3_nozero, 3)
trainM2 <- rbind(data1$train, 
               data2$train, data3$train)

validationM2 <- rbind(data1$validation,
                    data2$validation,
                    data3$validation)

testM2 <- rbind(data1$test,
                    data2$test,
                    data3$test)

```

# 2 (b) baseline accuracy

The trivial classifier will have a high accuracy when the original image has mostly clear areas (without clouds).
- Method 1 is preferred over Method 2.
- The way method 1 is constructed ensures that in training, validation and test sets, there are equal amounts of data from each image. 
- The table at the end of Method 2, however, suggests that the random selection tends to contain more data from one image than another.

```{r}
train_validation <- rbind(training, validation)
train_validationM2 <- rbind(trainM2,validationM2)
```

```{r}
baseline_preds <- 0
# validation accuracy
mean(baseline_preds == validation$expert_label)
# test accuracy
mean(baseline_preds == testing$expert_label)
```

```{r}
# c)
# changed: training_validation --> image1_nozero
image1_pca <- prcomp(image1_nozero[,c(7:11)], center = TRUE, scale. = TRUE)
autoplot(image1_pca, data = image1_nozero, loadings = TRUE, loadings.label = TRUE)
# Logistic 
train_pca <- prcomp(training[,c(7:11)], center = TRUE, scale. = TRUE)
PC1 <- train_pca$x[,1]
PC2 <- train_pca$x[,2]
PC3 <- train_pca$x[,3]
pca_train <- data.frame(expert_label = training$expert_label, PC1, PC2, PC3)
pca_logistic <- glm(expert_label ~ PC1 + PC2 + PC3,
                    data = pca_train,
                    family = "binomial")
testing_pca <- prcomp(testing[,c(7:11)], center = TRUE, scale. = TRUE)
PC1 <- testing_pca$x[,1]
PC2 <- testing_pca$x[,2]
PC3 <- testing_pca$x[,3]
testing.pca <- data.frame(expert_label = testing$expert_label, PC1, PC2, PC3)
y_hat_pca <- predict(pca_logistic, newdata = testing.pca, type = 'response')
y_hat_pca <- y_hat_pca > 0.5
y_hat_pca[which(y_hat_pca == FALSE)] <- 0
y_hat_pca[which(y_hat_pca == TRUE)] <- 1
mean(y_hat_pca  == testing$expert_label)
RA_mod <- glm(expert_label ~ RA_AN + RA_AF + RA_BF + RA_CF + RA_DF,
              data = train_validation,
              family = "binomial")
y_hat_RA <- predict(RA_mod, newdata = testing, type = 'response')
y_hat_RA <- y_hat_RA > 0.5
y_hat_RA[which(y_hat_RA == FALSE)] <- 0
y_hat_RA[which(y_hat_RA == TRUE)] <- 1
mean(y_hat_RA == testing$expert_label)

## PCA accuract only ~ 0.5?
```

# 2 d) generic CV function

```{r}
# d)
CVgeneric <- function(classifier, features, labels, K, loss) {
    folds <- createFolds(labels, k = K)
    error <- rep(0, K)
    for (i in 1: K) {
      test.feature <- features[folds[[i]],]
      train.feature <- features[-folds[[i]],]
      label.test <- labels[folds[[i]]]
      label.train <- labels[-folds[[i]]]
      
      mod_fit <- train(train.feature, label.train, method=classifier)
      y_hat = predict(mod_fit, newdata=test.feature)
      error[i] = loss(y_hat, label.test)
    }
    return (mean(error))
  }
```

# 3. Modeling


```{r}
# Logistic Regression Model 

# Split 1
CV <- function(data, K) {
    folds <- createFolds(1:nrow(data), k = K)
    accuracy <- data.frame(Folds = c(1:K), Accuracy_Rate = rep(0, K))
    for (i in 1: K) {
      test <- data[folds[[i]],]
      train.cv <- data[-folds[[i]],]
      
      mod_fit <- glm(expert_label ~ SD + CORR + NDAI,
                       data = train.cv,
                       family = "binomial")
      y_hat = predict(mod_fit, newdata=test)
      y_hat <- y_hat > 0.5
      y_hat[which(y_hat == FALSE)] <- 0
      y_hat[which(y_hat == TRUE)] <- 1
      accuracy[i,2] = mean(y_hat  == test$expert_label)
    }
    return (accuracy)
}

CV(train_validation, 5)

# Split 2
CV(train_validationM2, 5)
# qda, lda, svm, knn, random forest, neural network 
```

```{r}
# QDA

# Split 1
CV2 <- function(data, K) {
    folds <- createFolds(1:nrow(data), k = K)
    accuracy <- data.frame(Folds = c(1:K), Accuracy_Rate = rep(0, K))
    for (i in 1: K) {
      test <- data[folds[[i]],]
      train.cv <- data[-folds[[i]],]
      
      qda_mod1 <- qda(expert_label ~ SD + CORR + NDAI, 
                     data = train.cv)
      y_hat = predict(qda_mod1, test)
      
      accuracy[i,2] = mean(y_hat$class == test$expert_label)
    }
    return (accuracy)
}
CV2(train_validation, K = 5)

# Split 2
CV2(train_validationM2, K = 5)

```

```{r}
# LDA
# Split 1
# Split 1
CV3 <- function(data, K) {
    folds <- createFolds(1:nrow(data), k = K)
    accuracy <- data.frame(Folds = c(1:K), Accuracy_Rate = rep(0, K))
    for (i in 1: K) {
      test <- data[folds[[i]],]
      train.cv <- data[-folds[[i]],]
      
      qda_mod1 <- lda(expert_label ~ SD + CORR + NDAI, 
                     data = train.cv)
      y_hat = predict(qda_mod1, test)
      
      accuracy[i,2] = mean(y_hat$class == test$expert_label)
    }
    return (accuracy)
}

# Split 1
CV3(train_validation, K = 5)

# Split 2
CV3(train_validationM2, K = 5)
```

```{r}
# KNN

CV4 <- function(data, K, k_num) {
    folds <- createFolds(1:nrow(data), k = K)
    accuracy <- data.frame(Folds = c(1:K), Accuracy_Rate = rep(0, K))
    for (i in 1: K) {
      test <- data[folds[[i]],]
      train.cv <- data[-folds[[i]],]
      
      train_filtered <- train.cv[,c(4,5,6)]
      test_filtered <- test[,c(4,5,6)]
      
      y_hat <- knn(train_filtered, test_filtered, train.cv$expert_label, k = k_num)
      accuracy[i] = mean(y_hat == test$expert_label)
    }
    return (accuracy)
}

# Split 1
system.time(CV4(train_validation, K = 5, k_num = 10))

# Split 2
system.time(CV4(train_validationM2, K = 5, k_num = 10))
```

```{r eval = FALSE}
# SVM
# Split 1

# SVM no need to run, too slow

CV5 <- function(data, K) {
    folds <- createFolds(1:nrow(data), k = K)
    accuracy <- rep(0, K)
    for (i in 1: K) {
      test <- data[folds[[i]],]
      train.cv <- data[-folds[[i]],]
      
      qda_mod1 <- svm(expert_label ~ SD + CORR + NDAI, 
                     data = train.cv)
      y_hat = predict(qda_mod1, test)
      
      accuracy[i] = mean(y_hat$class == test$expert_label)
    }
    return (mean(accuracy))
}
CV5(train_validation, K = 5)

# https://rischanlab.github.io/SVM.html

# Split 2
CV5(train_validationM2, K = 5)

```


```{r}
# RandomForest
CV6 <- function(data, K, ntree) {
    folds <- createFolds(1:nrow(data), k = K)
    accuracy <- data.frame(Folds = c(1:K), Accuracy_Rate = rep(0, K))
    for (i in 1: K) {
      test <- data[folds[[i]],]
      train.cv <- data[-folds[[i]],]
      
      fit_randomForest <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = train.cv,
                   ntree = ntree)
      y_hat = predict(fit_randomForest, test)
      
      accuracy[i] = mean(y_hat == test$expert_label)
    }
    return (accuracy)
}

# Split 1
CV6(train_validation, K = 5, ntree = 32)

# Split 2
CV6(train_validationM2, K = 5, ntree=3)

```


#3 b)
```{r}
# Compute AUC for predicting Class with the model

# logistic regression
system.time(logistic_mod <- glm(expert_label ~ SD + CORR + NDAI,
                       data = train_validation,
                       family = "binomial"))
y_hat_logistic <- predict(logistic_mod, newdata = testing, type="response")

# https://stats.stackexchange.com/questions/10501/calculating-aupr-in-r
fg1 <- y_hat_logistic[testing$expert_label == 1]
bg1 <- y_hat_logistic[testing$expert_label == 0]

roc1 = roc.curve(fg1, bg1, curve = T)
pr1 = pr.curve(fg1, bg1, curve = T)
plot(roc1)
plot(pr1)
# y_hat_logistic <- y_hat_logistic > 0.5
# y_hat_logistic[which(y_hat_logistic == FALSE)] <- 0
# y_hat_logistic[which(y_hat_logistic == TRUE)] <- 1
# confusionMatrix(data = as.factor(y_hat_logistic), testing$expert_label)

# qda
qda_mod <- qda(expert_label ~ SD + CORR + NDAI, 
                     data = train_validation)
y_hat_qda = predict(qda_mod, testing)$posterior[,1]

fg2 <- y_hat_qda[testing$expert_label == 0]
bg2 <- y_hat_qda[testing$expert_label == 1]

roc2 = roc.curve(fg2, bg2, curve = T)
pr2 = pr.curve(fg2, bg2, curve = T)

# lda
lda_mod <- lda(expert_label ~ SD + CORR + NDAI, 
                     data = train_validation)
y_hat_lda = predict(lda_mod, testing)$posterior[,1]

fg3 <- y_hat_lda[testing$expert_label == 0]
bg3 <- y_hat_lda[testing$expert_label == 1]

roc3 = roc.curve(fg3, bg3, curve = T)
pr3 = pr.curve(fg3, bg3, curve = T)

# knn

train_filtered <- train_validation[,c(4,5,6)]
test_filtered <- testing[,c(4,5,6)]

y_hat_knn = knn(train_filtered, test_filtered, train_validation$expert_label, k = 10,
                     prob = TRUE)

y_hat_knn <- attr(y_hat_knn, "prob")

fg4 <- y_hat_knn[testing$expert_label == 0]
bg4 <- y_hat_knn[testing$expert_label == 1]

roc4 = roc.curve(fg4, bg4, curve = T)
pr4 = pr.curve(fg4, bg4, curve = T)



# random forests
system.time(fit_randomForest <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = train_validation, importance= T,
                   ntree = 3))

y_hat_rf <- predict(fit_randomForest, testing, type = "prob")
y_hat_rf = y_hat_rf[,1]

fg5 <- y_hat_rf[testing$expert_label == 0]
bg5 <- y_hat_rf[testing$expert_label == 1]

roc5 = roc.curve(fg5, bg5, curve = T)
pr5 = pr.curve(fg5, bg5, curve = T)

```

#3c
```{r}
# use three decision trees to examine three images
rpart.plot(rpart(expert_label ~ SD + CORR + NDAI, 
                   data = image1_nozero))

rpart.plot(rpart(expert_label ~ SD + CORR + NDAI, 
                   data = image2_nozero))

rpart.plot(rpart(expert_label ~ SD + CORR + NDAI, 
                   data = image3_nozero))
```

4. Diagnostics

a) 
- Random forest: accuracy converge to a certain value as ntree goes infinitely high
- Knn: same as above for k value

```{r eval = FALSE}
system.time(fit_randomForest <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = train_validation, importance= T,
                   ntree = 50))

mean_accuracy <- rep(0, 10)
n_tree <- c(1, 3, 5, 10, 15, 20, 30, 40, 50, 80, 100)
for (i in 1:length(n_tree)) {
  fit_randomForest <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = train_validation, importance= T,
                   ntree = n_tree[i])
  y_hat_randomforest <- predict(fit_randomForest, testing)
  mean_accuracy[i] = mean(y_hat == testing$expert_label)
}
```


b) extract misclassification data
```{r}
# Chosen classifier: random forest: use ntree = 32 for now NEED TO CHANGE
# split 1
randomforest_mod <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = train_validation,
                   ntree = 32)
randomforest_y_hat <- predict(randomforest_mod, testing)
misclassified1 <- testing[randomforest_y_hat != testing$expert_label,]
plot(misclassified1$x_coordinate, misclassified1$y_coordinate)
# more errors were made on the edges (when y coordinate value was low
boxplot(misclassified1$NDAI, train_validation$NDAI, names = c("Misclassified", "Train_Validation"), 
        horizontal = TRUE)
boxplot(misclassified1$CORR, train_validation$CORR, names = c("Misclassified", "Train_Validation"),
        horizontal = TRUE)
boxplot(misclassified1$SD, train_validation$SD, names = c("Misclassified", "Train_Validation"),
        horizontal = TRUE)
plot(misclassified1$expert_label)
# more errors were made by identifying an area to be clear when there is in fact cloud 

# split 2
randomforest_mod2 <- randomForest(expert_label ~ SD + CORR + NDAI, 
                   data = train_validationM2,
                   ntree = 32)
randomforest_y_hat2 <- predict(randomforest_mod2, testing)
misclassified2 <- testing[randomforest_y_hat2 != testing$expert_label,]
plot(misclassified2$x_coordinate, misclassified2$y_coordinate)
plot(misclassified2$expert_label)
# more errors were made by identifying an area to be clear when there is in fact cloud 
```

https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/

# 4 c)
```{r}

```









